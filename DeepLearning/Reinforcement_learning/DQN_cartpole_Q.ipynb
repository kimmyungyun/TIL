{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DQN_cartpole-Q.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "KvQfcJsAs18y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "from gym.envs.registration import register\n",
        "import math\n",
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6pkHGZo1Bzqg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.enable_eager_execution()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OBnqUkZT0ilt",
        "colab_type": "text"
      },
      "source": [
        "## 카트폴 게임"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6OLSfxVh5u2A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#installing dependencies\n",
        "!apt-get -qq -y install libnvtoolsext1 > /dev/null\n",
        "!ln -snf /usr/lib/x86_64-linux-gnu/libnvrtc-builtins.so.8.0 /usr/lib/x86_64-linux-gnu/libnvrtc-builtins.so\n",
        "!apt-get -qq -y install xvfb freeglut3-dev ffmpeg> /dev/null\n",
        "!pip -q install gym\n",
        "!pip -q install pyglet\n",
        "!pip -q install pyopengl\n",
        "!pip -q install pyvirtualdisplay"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YX2NCPpr3sDK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gym\n",
        "env = gym.make(\"CartPole-v0\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cLOdZ6B47Sb2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=0, size=(1024, 768))\n",
        "display.start()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vim7I4-y3-AA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.set_printoptions(threshold=np.inf)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2wHeBmlJB9tn",
        "colab_type": "text"
      },
      "source": [
        "## weight 모델 생성"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dkNWVLdaYhwx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "initializer = tf.contrib.layers.xavier_initializer()\n",
        "\n",
        "w1 = tf.Variable(initializer([4, 16]))\n",
        "w2 = tf.Variable(initializer([16, 2]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J0N-SBjwc5dl",
        "colab_type": "text"
      },
      "source": [
        "## Observation을 활용한 모델 적용"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HYBn-mbWal6O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "observation = env.reset()\n",
        "hypothesis1 = tf.matmul(np.array(np.reshape(observation, [-1, 4]), dtype=\"float32\"), w1)\n",
        "hypothesis1 = tf.nn.relu(hypothesis1, 0)\n",
        "hypothesis2 = tf.matmul(hypothesis1, w2)\n",
        "hypothesis2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cGA3lLA5c93n",
        "colab_type": "text"
      },
      "source": [
        "## 위의 작업을 함수로 정의"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pmoNE06IdAvr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict(observation):\n",
        "    hypothesis1 = tf.matmul(np.array(np.reshape(observation, [-1, 4]), dtype=\"float32\"), w1)\n",
        "    hypothesis1 = tf.nn.relu(hypothesis1, 0)\n",
        "    hypothesis2 = tf.matmul(hypothesis1, w2)\n",
        "    return hypothesis2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DnYA3ewYPf1a",
        "colab_type": "text"
      },
      "source": [
        "## 연관성이 적은 데이터를 활용하여 학습시키기 위해 데이터에서 일부만 선택해서 학습을 진행"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8IDFQHq1ikUs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_stack = np.empty(0).reshape(0, 4)\n",
        "x_stack = np.vstack([x_stack, observation])\n",
        "x_stack = np.vstack([x_stack, observation * 3])\n",
        "x_stack"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7nvCQNeiokER",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "from collections import deque\n",
        "REPLAY_MEMORY=50000\n",
        "\n",
        "MAX_EPISODE = 5000\n",
        "\n",
        "# minimum epsilon for epsilon greedy\n",
        "MIN_E = 0.0\n",
        "# epsilon will be `MIN_E` at `EPSILON_DECAYING_EPISODE`\n",
        "EPSILON_DECAYING_EPISODE = MAX_EPISODE * 0.01"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YFGCGhjBoaJG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def annealing_epsilon(episode: int, min_e: float, max_e: float, target_episode: int) -> float:\n",
        "    \"\"\"Return an linearly annealed epsilon\n",
        "    Epsilon will decrease over time until it reaches `target_episode`\n",
        "         (epsilon)\n",
        "             |\n",
        "    max_e ---|\\\n",
        "             | \\\n",
        "             |  \\\n",
        "             |   \\\n",
        "    min_e ---|____\\_______________(episode)\n",
        "                  |\n",
        "                 target_episode\n",
        "     slope = (min_e - max_e) / (target_episode)\n",
        "     intercept = max_e\n",
        "     e = slope * episode + intercept\n",
        "    Args:\n",
        "        episode (int): Current episode\n",
        "        min_e (float): Minimum epsilon\n",
        "        max_e (float): Maximum epsilon\n",
        "        target_episode (int): epsilon becomes the `min_e` at `target_episode`\n",
        "    Returns:\n",
        "        float: epsilon between `min_e` and `max_e`\n",
        "    \"\"\"\n",
        " \n",
        "    slope = (min_e - max_e) / (target_episode)\n",
        "    intercept = max_e\n",
        " \n",
        "    return max(min_e, slope * episode + intercept)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iE6iQHWvpgNY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#w값을 자동으로 수정하는 Optimizer 객체를 생성\n",
        "optimizer = tf.train.AdamOptimizer(0.001)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GMSGTPsulb_E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#큐 객체 생성\n",
        "dis = 0.99\n",
        "replay_buffer=deque()\n",
        "frameList = []\n",
        "costList=[]\n",
        "#5000번 반복해서 실행\n",
        "for step in range(5001):\n",
        "    print(\"=\"*100)\n",
        "    print(\"step:\",step)\n",
        "    print(\"=\"*100)\n",
        "    #게임 재시작\n",
        "    #env.reset() : 게임을 재시작\n",
        "    observation=env.reset() \n",
        "    #e 값을 리턴 받음\n",
        "    e=annealing_epsilon(step, MIN_E, 1.0, EPSILON_DECAYING_EPISODE)\n",
        "    \n",
        "    \n",
        "    for frame in range(200):\n",
        "        print(\"=\"*100)\n",
        "        predQ=predict(observation)\n",
        "        #np.random.rand(1): 난수를 0~1 사이 난수 1개 생성\n",
        "        #난수가 e보다 작으면 \n",
        "        if np.random.rand(1)<e:\n",
        "            #env.action_space.sample(): 0,1,2,3 중 하나의 액션 랜덤하게 리턴\n",
        "            action=env.action_space.sample()\n",
        "        else:\n",
        "            #predQ (예측한 Q값) 에서 가장 큰값을 가진 action을 선택\n",
        "            action=np.argmax(predQ)\n",
        "\n",
        "        print(\"action:\",action)\n",
        "\n",
        "        #env.step(action): action을 실행\n",
        "        next_observation, reward, done, info=env.step(action)\n",
        "                \n",
        "        if done:\n",
        "            #게임이 끝나면 reward는 -1\n",
        "            reward=-1\n",
        "            #게임이 끝나면 frame의 수를 저장\n",
        "            frameList.append([step,frame])\n",
        "\n",
        "        print(\"=\"*100)            \n",
        "        print(\"next_observation:\",next_observation)\n",
        "        print(\"reward:\",reward)\n",
        "        print(\"done:\",done)\n",
        "        print(\"info:\",info)\n",
        "        print(\"=\"*100)\n",
        "        #버퍼에 데이터 저장            \n",
        "        replay_buffer.append((observation,action,reward,next_observation,done))\n",
        "        #버퍼에 저장된 데이터의 개수가  REPLAY_MEMORY(50000) 초과이면\n",
        "        if len(replay_buffer) >REPLAY_MEMORY:\n",
        "            #맨처음에 저장된 데이터 삭제\n",
        "            replay_buffer.popleft()\n",
        "        \n",
        "        observation=next_observation\n",
        "        #게임이 끝나면 (done==True) 종료\n",
        "        if done==True:\n",
        "            break\n",
        "\n",
        "    print(\"=\"*100)            \n",
        "    print(\"CarPole Done!!: step:\",step,\":frame:\",frame)\n",
        "    print(\"=\"*100)            \n",
        "\n",
        "    if step%10==0:\n",
        "        #64번 반복\n",
        "        for index in range(64):\n",
        "            #random.sample(replay_buffer,10): reply_buffer에서 랜덤하게 10개의 데이터 선택\n",
        "            minibatch=random.sample(replay_buffer,10)\n",
        "            ###np.empty(0): 비어 있는 배열 생성\n",
        "            #reshape(0,4) : 비어있는 배열을 4칸으로 설정\n",
        "            x_stack=np.empty(0).reshape(0,4)\n",
        "            ###np.empty(0): 비어 있는 배열 생성\n",
        "            #reshape(0,2) : 비어있는 배열을 2칸으로 설정\n",
        "            y_stack=np.empty(0).reshape(0,2)\n",
        "\n",
        "            #minibatch에서 데이터를 꺼내서 대입\n",
        "            for batch_observation,batch_action,batch_reward,batch_next_observation,batch_done in minibatch:\n",
        "                #batch_observation 을 예측\n",
        "                predQ=predict(batch_observation)\n",
        "                #predQ를 numpy배열로 변환\n",
        "                Q=predQ.numpy()\n",
        "                #Qs의 action번째에 reward+dis*np.max(nextQ) 저장\n",
        "                #*~1 는 -2의 1승을 곱함 \n",
        "                #batch_done  True는 1이므로 *-2 곱하고\n",
        "\n",
        "                #*~1 는 -2의 0승을 곱함 \n",
        "                #batch_done False는 0 이므로 *-1 곱\n",
        "\n",
        "                #batch_done이 False면 값이 작아짐\n",
        "                Q[0,batch_action]=batch_reward+dis*np.max(predict(batch_next_observation))*~batch_done\n",
        "                #Q를 텐서플로우가 cost를 계산할 수있는 Tensor 타입으로 변환\n",
        "                Q=tf.convert_to_tensor(Q, np.float32)      \n",
        "                # print(\"=\"*100) \n",
        "                # print(\"batch_observation:\",batch_observation)           \n",
        "                # print(\"batch_next_observation:\",batch_next_observation)\n",
        "                # print(\"batch_reward:\",batch_reward)\n",
        "                # print(\"batch_done:\",batch_done)\n",
        "                # print(\"predQ:\",predQ)\n",
        "                # print(\"Q:\",Q)\n",
        "                # print(\"=\"*100)  \n",
        "                #np.vstack([x_stack, batch_observation]): x_stack에 batch_observation 추가\n",
        "                x_stack=np.vstack([x_stack, batch_observation])\n",
        "                ##np.vstack([y_stack,Q]): y_stack에 Q 추가\n",
        "                y_stack=np.vstack([y_stack,Q])\n",
        "                \n",
        "                \n",
        "                # print(\"=\"*100)\n",
        "                # print(\"x_stack:\",x_stack)\n",
        "                # print(\"y_stack:\",y_stack)\n",
        "                # print(\"=\"*100)\n",
        "\n",
        "            with tf.GradientTape() as tape:\n",
        "                #predict(x_stack): x_stack을 예측\n",
        "                predQ=predict(x_stack)\n",
        "                #predQ의 실제값이 저장된 y_stack과 차를 계산\n",
        "                # tf.reduce_mean(): 평균을 계산 합니다.\n",
        "                #tf.square: 제곱을 계산합니다\n",
        "                cost=tf.reduce_mean(tf.square(y_stack-predQ))\n",
        "                #cost,w0,w1,b0,b1 를 이용하여 cost가 0이 되는 w1,b1의 기울기, w0,b0의 기울기를 계산해서 grads에 대입 \n",
        "                # -> Backpropagation  실행\n",
        "                grads=tape.gradient(cost, [w1,w2])\n",
        "    \n",
        "                #grads에 저장된 w0,b0의 기울기와 w1,b1 기울기를 w0,b0와 w1,b1 에서 빼주고 새로운\n",
        "                #w0,b0와 w1,b1로 업데이트\n",
        "                optimizer.apply_gradients(grads_and_vars=zip(grads,[w1,w2]))\n",
        "                print(\"=\"*100)\n",
        "                print(\"cost:\",cost)\n",
        "                costList.append(cost.numpy())\n",
        "                print(\"=\"*100)            "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gzjO4DUK6eP6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#cost를 그래프로 그림\n",
        "plt.plot(range(len(costList)), costList, marker='.', c='red', label=\"cost\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6WIhiC-9CheY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.animation\n",
        "import numpy as np\n",
        "from IPython.display import HTML"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lDw-qphl6rhv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#animationFrame: Cart Pole 게임의 각 장면의 이미지의 RGB 값을 저장할 리스트\n",
        "animationFrame = []\n",
        "\n",
        "#env.reset() : 게임을 재시작\n",
        "observation=env.reset()   \n",
        "#env.render(mode = 'rgb_array') :Cart Pole 게임의 각 장면의 이미지를 numpy 배열로 출력\n",
        "#                                이미지는 rgb 값이 출력\n",
        "\n",
        "#animationFrame.append : 이미지를 animationFrame에 추가\n",
        "animationFrame.append(env.render(mode = 'rgb_array'))\n",
        "#200 번 반복해서 게임 진행 \n",
        "for frame in range(200):\n",
        "    print(\"=\"*100) \n",
        "    print(\"frame:\",frame)\n",
        "    print(\"=\"*100) \n",
        "    \n",
        "    predQ=predict(observation)\n",
        "    print(\"predQ:\",predQ)\n",
        "    #np.amax(Q[state_0]) :  Q 테이블 state_0 인덱스의 최대값이 리턴됨\n",
        "    #최대값을 변수 m에 저장\n",
        "    m = np.amax(predQ)\n",
        "    print(\"m:\",m)\n",
        "    #m이 0이면 (Q 테이블 state_0 인덱스에 action정보가 저장되어 있지 않음)\n",
        "    if m==0:\n",
        "        #pr.choice([0,1]) : 0,1 중하나를 임의로 선택\n",
        "        action=pr.choice([0,1])        \n",
        "    else:\n",
        "        #np.argmax(Q[state_0]): Q[state_0] 의 최대값을 리턴\n",
        "        action=np.argmax(predQ)\n",
        "\n",
        "    print(\"action:\",action) \n",
        "\n",
        "    #env.step(action): Cart Pole을 action 방향으로 이동\n",
        "    new_observation, reward, done, info = env.step(action)\n",
        "    #env.step(action): action을 실행\n",
        "    next_observation, reward, done, info=env.step(action)\n",
        "    \n",
        "    print(\"next_observation:\",next_observation)\n",
        "    print(\"reward:\",reward)\n",
        "    print(\"done:\",done)\n",
        "    print(\"info:\",info)\n",
        "    #done이 True면 게임 종료\n",
        "    if done==True:\n",
        "        break;\n",
        "\n",
        "    #env.render(mode = 'rgb_array') :Cart Pole 게임의 각 장면의 이미지를 numpy 배열로 출력\n",
        "    #                                이미지는 rgb 값이 출력\n",
        "\n",
        "    #animationFrame.append : 이미지를 animationFrame에 추가\n",
        "    animationFrame.append(env.render(mode = 'rgb_array'))\n",
        "    observation=next_observation\n",
        "print(\"=\"*100)    \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qAkY7GfRh-O0",
        "colab_type": "text"
      },
      "source": [
        "## 카트폴 게임 이미지를 저장하고, 보여주기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1gG8dAZzCtWw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def animate(index):\n",
        "    patch.set_data(animationFrame[index])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tj7lxJHnquhm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "len(animationFrame)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MTfIZ3rKDezQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(8, 5))\n",
        "patch = plt.imshow(animationFrame[0])\n",
        "ani = matplotlib.animation.FuncAnimation(plt.gcf(), \n",
        "                                         animate, \n",
        "                                         frames=len(animationFrame),\n",
        "                                         interval = 50)\n",
        "\n",
        "HTML(ani.to_jshtml())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G1mWrsZID7pS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
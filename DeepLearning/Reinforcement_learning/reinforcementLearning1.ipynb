{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "reinforcementLearning1.ipynb",
      "provenance": [],
      "private_outputs": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "KvQfcJsAs18y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gym\n",
        "from gym.envs.registration import register\n",
        "import numpy as np\n",
        "import random as pr"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BL8eT3kp3nKv",
        "colab_type": "text"
      },
      "source": [
        "## 프로즌레이크 게임"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kO87FcU4tXKZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "register(\n",
        "    id=\"FrozenLake-v1\",\n",
        "    entry_point=\"gym.envs.toy_text:FrozenLakeEnv\",\n",
        "    kwargs={\n",
        "        \"map_name\":\"4x4\",\n",
        "        \"is_slippery\":False\n",
        "    }\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AJMX3PZyxgR7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "env = gym.make(\"FrozenLake-v1\")\n",
        "env.render()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1W8tAyPL0Cxu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "position= env.reset()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tn8BsIqZkVFX",
        "colab_type": "text"
      },
      "source": [
        "## 왼쪽 아래 오른쪽 위 순서로 OneHotEncoding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OUzvQiNG0LUd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Q = np.array([\n",
        "              [0, 0, 1, 0],\n",
        "              [0, 0, 0, 0]\n",
        "])\n",
        "Q\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g7LkZs3ukcDH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "m = np.amax(Q[0,:])\n",
        "print(f\"m : {m}\")\n",
        "action = np.argmax(Q[0, :])\n",
        "print(f\"action : {action}\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g83fp1w2nXkB",
        "colab_type": "text"
      },
      "source": [
        "## 어디로 이동해야할지 모를 경우 랜덤으로 갈 수 있도록"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HZFohULtndPD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "action = pr.choice([0, 1, 2, 3])\n",
        "print(f\"action : {action}\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2yHdCoBUvZ6E",
        "colab_type": "text"
      },
      "source": [
        "## Q 함수를 활용한 Frozen Lake"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PFYbCjXtvc7h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Q = np.zeros([16, 4])\n",
        "\n",
        "for index in range(50):\n",
        "    print(f\"index : \", index)\n",
        "    position = env.reset()\n",
        "\n",
        "    while True:\n",
        "        env.render()\n",
        "        print()\n",
        "\n",
        "        m = np.amax(Q[position, :])\n",
        "        if m == 1:\n",
        "            action = np.argmax(Q[position, :])\n",
        "        else:\n",
        "            action = pr.choice([0, 1, 2, 3])\n",
        "\n",
        "        new_position, reward, done, info = env.step(action)\n",
        "\n",
        "        print(f\"new_position : {new_position}, reward : {reward}, done : {done}, info : {info}\")\n",
        "\n",
        "        Q[position, action ] = reward + np.max(Q[new_position, :])\n",
        "\n",
        "        position = new_position\n",
        "\n",
        "        if done == True:\n",
        "            break\n",
        "\n",
        "    print(f\"Q: {Q}\")\n",
        "    print(\"=\"* 100)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
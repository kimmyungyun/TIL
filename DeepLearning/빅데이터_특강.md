## 특강

## 엔트로피

- 물리에서 엔트로피는 혼잡 정도를 나타낸다

- 물리에서 가장 안정된 것이 1, 가장 불안정한 것이 0이다

- 그런데 머신러닝의 분류(classification) 에서는 가장 혼잡한 0이 가장 분류가 잘되는 것이다

  

## 정보 증가량(Information Gain)

- 해당 속성의 영향력이 높다

  - 값이 큰 것이 영향력이 높다는 것이다 
  - 타겟 값에 대해서 분류를 잘한다

- ![](assets/eq5.png)

- c1의 경우 1 속성의 값 중 타겟 밸류 값이 나올 것을 말한다

- 공식에서 확률을 곱해주는 이유는 불균등한 데이터를 균등하게 영향력을 끼치게 할 수 있도록 보정하는 개념

  - Weight (가중치) 개념으로 본다
  - 그렇지만 요새는 기울기의 개념으로 보는 경우가 더 많다

- 예제 코드

  ```python
  import math
  c1_entropy = -1/13 * math.log(1/13, 2) - 12/13 * math.log(12/13, 2)
  c2_entropy = -4/17 * math.log(4/17, 2) - 13/17 * math.log(13/17, 2)
  p_entropy = -16/30 * math.log(16/30, 2) - 14/30 * math.log(14/30, 2)
  
  p_entropy = p_entropy - (13/30 * c1_entropy + 17/30 * c2_entropy)
  p_entropy
  ```

  

## Decision Tree

- 현재 보고 있는 데이터셋 안에서 타겟값을 봤을 때 타겟 값이 모두 같은 값으로 되어 있으면 멈춘다

- 보통 Yes냐 No냐가 서로 뭉쳐있게 만드려고 노력

- 순서

  1. 각각의 특징에 대해서 Entropy를 구함

  2. Information Gain을 기준으로 나누기 시작한다

  3. IG를 기준으로 나눈 것으로 타겟값을 확인하는데 모두 같은 값이 라면 해당 노드에서 브랜치는 스탑

  4. 다른 값이 존재한다면 다시 IG를 구해서 조각을 낸다 (3, 4번 항목 반복)

- 

## 데이터 전처리

- 숫자의 경우 전처리를 할 때 보통 구간화를 시켜준다
  - 예를 들어, 나이, 돈, 시간 등과 같은 것

- 구간화를 시킬 때 기준은 보통 현업에서는 전문가로부터 정보를 취득하여 그것을 또 대표값을 구해서 그것으로 기준을 선택하는 것이 보통



## 선형판별 함수

- 선형 선분을 찾는다
- 방식
  1. 실제 데이터를 갖고  있어야 한다
  2. 임의의 값으로 만든 모델에 값을 넣어서 타겟값과 비교
  3. 타겟값에서 예측값을 빼서 값을 확인
  4. 오차를 줄여가는 방식으로 모델을 바꾼다
  5. 위의 2~4를 반복한다



## 로지스틱 회귀분석

- 회귀 분석인데 분류문제에서 써먹음
- 풀어내면 logistic Curve가 나옴



## 단어

- N차원의 특징을 분류할 때 N-1 차원의 초평면으로 분류가 되는 것이다
- Occam's Razor
  - Simple is best

- 가중치의 합(Weighted sum)
  - 기울기의 합으로 볼 수 있다
  - 각각의 점들은 자신들을 구분하는 선분을 가지고 있는데 그 모든 점들이 가지고 있는 선분의 기울기를 더해서 잘 구분하는 기울기를 구하는 것

## 반드시 확인해야 할 것

- 모델을 통해 예측을 했으면 계층 확률 추정을 수행해야 한다
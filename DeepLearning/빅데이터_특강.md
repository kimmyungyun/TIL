## 특강

## 엔트로피

- 물리에서 엔트로피는 혼잡 정도를 나타낸다

- 물리에서 가장 안정된 것이 1, 가장 불안정한 것이 0이다

- 그런데 머신러닝의 분류(classification) 에서는 가장 혼잡한 0이 가장 분류가 잘되는 것이다

  

## 정보 증가량(Information Gain)

- 해당 속성의 영향력이 높다

  - 값이 큰 것이 영향력이 높다는 것이다 
  - 타겟 값에 대해서 분류를 잘한다

- $$
  \mathbf{IG} = entropy(parents) - [p(c_1) \times entropy(c1) + p(c_2) \times entropy(c_2) + \cdots{}]
  $$

- c1의 경우 1 속성의 값 중 타겟 밸류 값이 나올 것을 말한다

- 공식에서 확률을 곱해주는 이유는 불균등한 데이터를 균등하게 영향력을 끼치게 할 수 있도록 보정하는 개념

  - Weight (가중치) 개념으로 본다
  - 그렇지만 요새는 기울기의 개념으로 보는 경우가 더 많다

- 예제 코드

  ```python
  import math
  c1_entropy = -1/13 * math.log(1/13, 2) - 12/13 * math.log(12/13, 2)
  c2_entropy = -4/17 * math.log(4/17, 2) - 13/17 * math.log(13/17, 2)
  p_entropy = -16/30 * math.log(16/30, 2) - 14/30 * math.log(14/30, 2)
  
  p_entropy = p_entropy - (13/30 * c1_entropy + 17/30 * c2_entropy)
  p_entropy
  ```

  

## Decision Tree

- 현재 보고 있는 데이터셋 안에서 타겟값을 봤을 때 타겟 값이 모두 같은 값으로 되어 있으면 멈춘다

- 보통 Yes냐 No냐가 서로 뭉쳐있게 만드려고 노력

- 순서

  1. 각각의 특징에 대해서 Entropy를 구함

  2. Information Gain을 기준으로 나누기 시작한다

  3. IG를 기준으로 나눈 것으로 타겟값을 확인하는데 모두 같은 값이 라면 해당 노드에서 브랜치는 스탑

  4. 다른 값이 존재한다면 다시 IG를 구해서 조각을 낸다 (3, 4번 항목 반복)

- 

## 데이터 전처리

- 숫자의 경우 전처리를 할 때 보통 구간화를 시켜준다
  - 예를 들어, 나이, 돈, 시간 등과 같은 것

- 구간화를 시킬 때 기준은 보통 현업에서는 전문가로부터 정보를 취득하여 그것을 또 대표값을 구해서 그것으로 기준을 선택하는 것이 보통
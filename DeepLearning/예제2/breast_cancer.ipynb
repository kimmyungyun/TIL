{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.model_selection import train_test_split #데이터 셋을 train set과 test set으로 분리할 수 있는 클래스를 제공\n",
    "from sklearn.model_selection import StratifiedKFold # Stratified k fold cross validation을 사용하기 위한 모듈\n",
    "# Kfold 모듈과는 다르게 각 fold 내 데이터의 클래스 비율을 일정하게 유지\n",
    "from sklearn.model_selection import cross_val_score # cross validation 결과의 정확도를 측정하기 위한 모듈\n",
    "\n",
    "from sklearn.metrics import confusion_matrix #분석 결과의 confusion matrix를  추출하기 위한 모듈\n",
    "from sklearn.metrics import accuracy_score #분석 결과의 accuracy를 측정하기 위한 모듈\n",
    "from sklearn.metrics import classification_report #분석 결과의 recall, precision, f-measure를 측정하기 위한 모듈\n",
    "from sklearn.metrics import roc_auc_score # roc 곡선 아래 넓이를 구하기 위한 모듈\n",
    "from sklearn.metrics import mean_squared_error #분석 결과의 MSE를 구하기 위한 모듈"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = datasets.load_breast_cancer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.799e+01, 1.038e+01, 1.228e+02, ..., 2.654e-01, 4.601e-01,\n",
       "        1.189e-01],\n",
       "       [2.057e+01, 1.777e+01, 1.329e+02, ..., 1.860e-01, 2.750e-01,\n",
       "        8.902e-02],\n",
       "       [1.969e+01, 2.125e+01, 1.300e+02, ..., 2.430e-01, 3.613e-01,\n",
       "        8.758e-02],\n",
       "       ...,\n",
       "       [1.660e+01, 2.808e+01, 1.083e+02, ..., 1.418e-01, 2.218e-01,\n",
       "        7.820e-02],\n",
       "       [2.060e+01, 2.933e+01, 1.401e+02, ..., 2.650e-01, 4.087e-01,\n",
       "        1.240e-01],\n",
       "       [7.760e+00, 2.454e+01, 4.792e+01, ..., 0.000e+00, 2.871e-01,\n",
       "        7.039e-02]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = data.data\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "악성 = 0\n",
    "\n",
    "양성 = 1\n",
    "\n",
    "데이터의 label은 벡터이기 때문에 소문자로 변수명을 적음으로써 구분한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,\n",
       "       1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0,\n",
       "       1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,\n",
       "       0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1,\n",
       "       1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0,\n",
       "       1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1,\n",
       "       1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0,\n",
       "       0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "       0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n",
       "       1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1,\n",
       "       1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1,\n",
       "       1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
       "       1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,\n",
       "       1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,\n",
       "       1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = data.target\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Holdout 방식의 Train, test set 구성 방식"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train size :  455  test size :  114\n"
     ]
    }
   ],
   "source": [
    "print(\"train size : \",len(X_train), \" test size : \", len(X_test) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
       "                       max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                       random_state=None, splitter='best')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = DecisionTreeClassifier()\n",
    "clf.fit(X_train, y_train)\n",
    "clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class 0에 대해서 하는 것이기 때문에 0이 positive이다\n",
    "\n",
    "|        |           | 예측값    | 예측값    |\n",
    "| ------ | --------- | --------- | --------- |\n",
    "|        |           | Class = yes | Class = no |\n",
    "| 실제값 | Class = yes |    TP(true positive)       |    FN(false negative)      |\n",
    "| 실제값 | Class = no |    FP(false positive)       |     TN(true negative)      |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n",
      " [[39  2]\n",
      " [ 7 66]]\n"
     ]
    }
   ],
   "source": [
    "print('Confusion Matrix : \\n', confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|        |           | 예측값    | 예측값    |\n",
    "| ------ | --------- | --------- | --------- |\n",
    "|        |           | Class = 0 | Class = 1 |\n",
    "| 실제값 | Class = 0 |    39       |     2      |\n",
    "| 실제값 | Class = 1 |    7       |     66      |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : \n",
      " 0.9210526315789473\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy : \\n', accuracy_score(y_test, y_pred, normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy 는 맞은 비율을 이야기한다\n",
    "\n",
    "$$\n",
    "Accuracy = \\frac{TP+TN}{TP+TN+FP+FN}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.95      0.90        41\n",
      "           1       0.97      0.90      0.94        73\n",
      "\n",
      "    accuracy                           0.92       114\n",
      "   macro avg       0.91      0.93      0.92       114\n",
      "weighted avg       0.93      0.92      0.92       114\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Classification Report : \\n', classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision\n",
    "\n",
    "- 모델이 검출한 데이터 중 올바르게 검출된 데이터의 비율\n",
    "- 0(관심있는 클래스)이라고 예측한 것 중에 진짜 0의 비율\n",
    "\n",
    "$$\n",
    "Precision = \\frac{a}{a+c}\n",
    "$$\n",
    "\n",
    "## Recall\n",
    "\n",
    "- 실제 해당 데이터 중 모델이 올바르게 검출한 데이터의 비율\n",
    "- 진짜 0(관심있는 클래스) 중에서 맞춘 비율\n",
    "\n",
    "$$\n",
    "Recall = \\frac{a}{a+b}\n",
    "$$\n",
    "\n",
    "## F-Measure\n",
    "\n",
    "- 두 수치(Precision, Recall)의 trade-off관계를 통합해 하나의 수치로 정확도 도출\n",
    "\n",
    "$$\n",
    "F-measure(F) = \\frac{2a}{2a + b + c} = \\frac{2rp}{r+p}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC : \n",
      " 0.9276645506181089\n"
     ]
    }
   ],
   "source": [
    "print('AUC : \\n', roc_auc_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TPR = 민감도\n",
    "- 진짜 Yes 중에서 Yes 라고 예측한 비율\n",
    "\n",
    "FPR = 특이도\n",
    "- 진짜 No 중에서 Yes 라고 예측한 비율\n",
    "\n",
    "ROC Curve\n",
    "- Y축이 TPR, X축이 FPR로 구성되어있는 것\n",
    "\n",
    "잘 예측하지 못했다면, AUC 면적이 작고, 잘 예측했다면 AUC 면적이 크다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error : \n",
      " 0.07894736842105263\n"
     ]
    }
   ],
   "source": [
    "print('Mean Squared Error : \\n', mean_squared_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean squared Error\n",
    "- 오차를 제곱한 값들의 평균\n",
    "\n",
    "$$\n",
    "MSE = \\frac{\\sum_{i=1}^{n}{(\\hat{y}-y_i)^2}}{n}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StratifiedKFold(n_splits=10, random_state=None, shuffle=False)\n"
     ]
    }
   ],
   "source": [
    "skf = StratifiedKFold(n_splits = 10)\n",
    "skf.get_n_splits(X, y)\n",
    "print(skf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "StratifiedKFold(n_splits = 10)\n",
    "- 9/10은 trainset, 1/10은 testset\n",
    "- 10번 반복한다\n",
    "- 모든 Data에 대해서 성능을 계산한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set :  [ 25  26  27  28  29  30  31  32  33  34  35  36  38  39  40  41  42  43\n",
      "  44  45  47  53  54  56  57  62  64  65  70  72  73  75  77  78  82  83\n",
      "  85  86  87  91  94  95  99 100 101 102 103 104 105 106 107 108 109 110\n",
      " 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128\n",
      " 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146\n",
      " 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164\n",
      " 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182\n",
      " 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200\n",
      " 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218\n",
      " 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236\n",
      " 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254\n",
      " 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272\n",
      " 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290\n",
      " 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308\n",
      " 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326\n",
      " 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344\n",
      " 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362\n",
      " 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380\n",
      " 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398\n",
      " 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416\n",
      " 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434\n",
      " 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452\n",
      " 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470\n",
      " 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488\n",
      " 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506\n",
      " 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524\n",
      " 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542\n",
      " 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560\n",
      " 561 562 563 564 565 566 567 568]\n",
      "Test set :  [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 37 46 48 49 50 51 52 55 58 59 60 61 63 66 67 68 69 71 74 76 79 80 81\n",
      " 84 88 89 90 92 93 96 97 98]\n",
      "Train set :  [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  37  46  48  49  50  51  52  54  55  56  57\n",
      "  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72  73  74  75\n",
      "  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90  91  92  93\n",
      "  94  95  96  97  98  99 100 105 108 117 118 119 121 122 126 127 129 131\n",
      " 132 134 135 138 141 146 153 154 155 156 157 158 159 160 161 162 163 164\n",
      " 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182\n",
      " 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200\n",
      " 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218\n",
      " 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236\n",
      " 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254\n",
      " 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272\n",
      " 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290\n",
      " 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308\n",
      " 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326\n",
      " 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344\n",
      " 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362\n",
      " 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380\n",
      " 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398\n",
      " 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416\n",
      " 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434\n",
      " 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452\n",
      " 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470\n",
      " 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488\n",
      " 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506\n",
      " 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524\n",
      " 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542\n",
      " 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560\n",
      " 561 562 563 564 565 566 567 568]\n",
      "Test set :  [ 25  26  27  28  29  30  31  32  33  34  35  36  38  39  40  41  42  43\n",
      "  44  45  47  53 101 102 103 104 106 107 109 110 111 112 113 114 115 116\n",
      " 120 123 124 125 128 130 133 136 137 139 140 142 143 144 145 147 148 149\n",
      " 150 151 152]\n",
      "Train set :  [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "  55  58  59  60  61  63  66  67  68  69  71  74  76  79  80  81  84  88\n",
      "  89  90  92  93  96  97  98 100 101 102 103 104 105 106 107 108 109 110\n",
      " 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128\n",
      " 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146\n",
      " 147 148 149 150 151 152 156 161 162 164 167 168 171 172 177 180 181 182\n",
      " 184 186 190 193 194 196 197 198 199 201 202 203 205 207 210 212 213 214\n",
      " 215 218 219 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236\n",
      " 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254\n",
      " 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272\n",
      " 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290\n",
      " 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308\n",
      " 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326\n",
      " 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344\n",
      " 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362\n",
      " 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380\n",
      " 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398\n",
      " 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416\n",
      " 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434\n",
      " 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452\n",
      " 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470\n",
      " 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488\n",
      " 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506\n",
      " 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524\n",
      " 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542\n",
      " 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560\n",
      " 561 562 563 564 565 566 567 568]\n",
      "Test set :  [ 54  56  57  62  64  65  70  72  73  75  77  78  82  83  85  86  87  91\n",
      "  94  95  99 153 154 155 157 158 159 160 163 165 166 169 170 173 174 175\n",
      " 176 178 179 183 185 187 188 189 191 192 195 200 204 206 208 209 211 216\n",
      " 217 220 221]\n",
      "Train set :  [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
      "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
      "  90  91  92  93  94  95  96  97  98  99 101 102 103 104 106 107 109 110\n",
      " 111 112 113 114 115 116 120 123 124 125 128 130 133 136 137 139 140 142\n",
      " 143 144 145 147 148 149 150 151 152 153 154 155 157 158 159 160 163 164\n",
      " 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182\n",
      " 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200\n",
      " 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218\n",
      " 219 220 221 223 229 230 233 236 237 239 244 250 252 253 254 255 256 257\n",
      " 258 259 260 261 262 263 264 265 272 274 277 280 282 283 287 288 289 290\n",
      " 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308\n",
      " 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326\n",
      " 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344\n",
      " 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362\n",
      " 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380\n",
      " 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398\n",
      " 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416\n",
      " 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434\n",
      " 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452\n",
      " 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470\n",
      " 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488\n",
      " 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506\n",
      " 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524\n",
      " 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542\n",
      " 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560\n",
      " 561 562 563 564 565 566 567 568]\n",
      "Test set :  [100 105 108 117 118 119 121 122 126 127 129 131 132 134 135 138 141 146\n",
      " 156 161 162 222 224 225 226 227 228 231 232 234 235 238 240 241 242 243\n",
      " 245 246 247 248 249 251 266 267 268 269 270 271 273 275 276 278 279 281\n",
      " 284 285 286]\n",
      "Train set :  [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
      "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
      "  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
      " 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125\n",
      " 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143\n",
      " 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161\n",
      " 162 163 165 166 169 170 173 174 175 176 178 179 183 185 187 188 189 191\n",
      " 192 195 200 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218\n",
      " 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236\n",
      " 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254\n",
      " 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272\n",
      " 273 274 275 276 277 278 279 280 281 282 283 284 285 286 297 300 302 317\n",
      " 321 323 328 329 330 332 333 334 335 336 337 338 339 340 341 342 343 344\n",
      " 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362\n",
      " 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380\n",
      " 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398\n",
      " 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416\n",
      " 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434\n",
      " 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452\n",
      " 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470\n",
      " 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488\n",
      " 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506\n",
      " 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524\n",
      " 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542\n",
      " 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560\n",
      " 561 562 563 564 565 566 567 568]\n",
      "Test set :  [164 167 168 171 172 177 180 181 182 184 186 190 193 194 196 197 198 199\n",
      " 201 202 203 287 288 289 290 291 292 293 294 295 296 298 299 301 303 304\n",
      " 305 306 307 308 309 310 311 312 313 314 315 316 318 319 320 322 324 325\n",
      " 326 327 331]\n",
      "Train set :  [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
      "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
      "  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
      " 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125\n",
      " 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143\n",
      " 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161\n",
      " 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179\n",
      " 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197\n",
      " 198 199 200 201 202 203 204 206 208 209 211 216 217 220 221 222 224 225\n",
      " 226 227 228 231 232 234 235 238 240 241 242 243 245 246 247 248 249 251\n",
      " 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272\n",
      " 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290\n",
      " 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308\n",
      " 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326\n",
      " 327 328 329 330 331 335 337 339 343 351 352 353 365 366 368 369 370 372\n",
      " 373 379 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398\n",
      " 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416\n",
      " 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434\n",
      " 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452\n",
      " 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470\n",
      " 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488\n",
      " 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506\n",
      " 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524\n",
      " 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542\n",
      " 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560\n",
      " 561 562 563 564 565 566 567 568]\n",
      "Test set :  [205 207 210 212 213 214 215 218 219 223 229 230 233 236 237 239 244 250\n",
      " 252 253 254 332 333 334 336 338 340 341 342 344 345 346 347 348 349 350\n",
      " 354 355 356 357 358 359 360 361 362 363 364 367 371 374 375 376 377 378\n",
      " 380 381 382]\n",
      "Train set :  [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
      "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
      "  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
      " 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125\n",
      " 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143\n",
      " 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161\n",
      " 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179\n",
      " 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197\n",
      " 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215\n",
      " 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233\n",
      " 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251\n",
      " 252 253 254 266 267 268 269 270 271 273 275 276 278 279 281 284 285 286\n",
      " 287 288 289 290 291 292 293 294 295 296 298 299 301 303 304 305 306 307\n",
      " 308 309 310 311 312 313 314 315 316 318 319 320 321 322 323 324 325 326\n",
      " 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344\n",
      " 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362\n",
      " 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380\n",
      " 381 382 385 389 392 393 400 408 414 417 427 428 429 430 431 432 433 434\n",
      " 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452\n",
      " 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470\n",
      " 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488\n",
      " 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506\n",
      " 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524\n",
      " 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542\n",
      " 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560\n",
      " 561 562 563 564 565 566 567 568]\n",
      "Test set :  [255 256 257 258 259 260 261 262 263 264 265 272 274 277 280 282 283 297\n",
      " 300 302 317 383 384 386 387 388 390 391 394 395 396 397 398 399 401 402\n",
      " 403 404 405 406 407 409 410 411 412 413 415 416 418 419 420 421 422 423\n",
      " 424 425 426]\n",
      "Train set :  [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
      "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
      "  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
      " 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125\n",
      " 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143\n",
      " 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161\n",
      " 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179\n",
      " 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197\n",
      " 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215\n",
      " 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233\n",
      " 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251\n",
      " 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269\n",
      " 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287\n",
      " 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305\n",
      " 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 322 324 325\n",
      " 326 327 331 332 333 334 336 338 340 341 342 344 345 346 347 348 349 350\n",
      " 354 355 356 357 358 359 360 361 362 363 364 367 371 374 375 376 377 378\n",
      " 380 381 382 383 384 386 387 388 389 390 391 392 393 394 395 396 397 398\n",
      " 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416\n",
      " 417 418 419 420 421 422 423 424 425 426 430 432 433 435 441 444 446 449\n",
      " 451 460 461 468 475 476 477 478 479 480 481 482 483 484 485 486 487 488\n",
      " 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506\n",
      " 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524\n",
      " 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542\n",
      " 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560\n",
      " 561 562 563 564 565 566 567 568]\n",
      "Test set :  [321 323 328 329 330 335 337 339 343 351 352 353 365 366 368 369 370 372\n",
      " 373 379 385 427 428 429 431 434 436 437 438 439 440 442 443 445 447 448\n",
      " 450 452 453 454 455 456 457 458 459 462 463 464 465 466 467 469 470 471\n",
      " 472 473 474]\n",
      "Train set :  [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
      "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
      "  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
      " 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125\n",
      " 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143\n",
      " 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161\n",
      " 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179\n",
      " 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197\n",
      " 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215\n",
      " 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233\n",
      " 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251\n",
      " 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269\n",
      " 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287\n",
      " 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305\n",
      " 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323\n",
      " 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341\n",
      " 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359\n",
      " 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377\n",
      " 378 379 380 381 382 383 384 385 386 387 388 390 391 394 395 396 397 398\n",
      " 399 401 402 403 404 405 406 407 409 410 411 412 413 415 416 418 419 420\n",
      " 421 422 423 424 425 426 427 428 429 431 434 436 437 438 439 440 442 443\n",
      " 445 447 448 450 452 453 454 455 456 457 458 459 462 463 464 465 466 467\n",
      " 469 470 471 472 473 474 489 492 498 499 501 503 509 512 514 516 517 521\n",
      " 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542\n",
      " 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560\n",
      " 561 562 563 564 565 566 567 568]\n",
      "Test set :  [389 392 393 400 408 414 417 430 432 433 435 441 444 446 449 451 460 461\n",
      " 468 475 476 477 478 479 480 481 482 483 484 485 486 487 488 490 491 493\n",
      " 494 495 496 497 500 502 504 505 506 507 508 510 511 513 515 518 519 520\n",
      " 522 523 524]\n",
      "Train set :  [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
      "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
      "  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
      " 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125\n",
      " 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143\n",
      " 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161\n",
      " 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179\n",
      " 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197\n",
      " 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215\n",
      " 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233\n",
      " 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251\n",
      " 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269\n",
      " 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287\n",
      " 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305\n",
      " 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323\n",
      " 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341\n",
      " 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359\n",
      " 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377\n",
      " 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395\n",
      " 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413\n",
      " 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431\n",
      " 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449\n",
      " 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467\n",
      " 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485\n",
      " 486 487 488 490 491 493 494 495 496 497 500 502 504 505 506 507 508 510\n",
      " 511 513 515 518 519 520 522 523 524]\n",
      "Test set :  [489 492 498 499 501 503 509 512 514 516 517 521 525 526 527 528 529 530\n",
      " 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548\n",
      " 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566\n",
      " 567 568]\n"
     ]
    }
   ],
   "source": [
    "for train_index, test_index in skf.split(X, y):\n",
    "    print('Train set : ', train_index)\n",
    "    print('Test set : ', test_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K Fold Cross Validation Score\n",
      " [0.94736842 0.87719298 0.92982456 0.85964912 0.92982456 0.89473684\n",
      " 0.89473684 0.94736842 0.9122807  0.98214286]\n",
      "Average Accuracy :  0.9175125313283207\n"
     ]
    }
   ],
   "source": [
    "clf = DecisionTreeClassifier()\n",
    "scores = cross_val_score(clf, X, y, cv = skf)\n",
    "print('K Fold Cross Validation Score\\n', scores)\n",
    "print(\"Average Accuracy : \", scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K Fold Cross Validation Score\n",
      " [0.94736842 0.9122807  0.94736842 0.92982456 0.9122807  0.92982456\n",
      " 0.89473684 0.92982456 0.9122807  0.89285714]\n",
      "Average Accuracy :  0.9208646616541353\n"
     ]
    }
   ],
   "source": [
    "skf = StratifiedKFold(n_splits = 10, shuffle = True)\n",
    "skf.get_n_splits(X, y)\n",
    "    \n",
    "clf = DecisionTreeClassifier()\n",
    "scores = cross_val_score(clf, X, y, cv = skf)\n",
    "print('K Fold Cross Validation Score\\n', scores)\n",
    "print(\"Average Accuracy : \", scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 데이터셋 준비는 shuffle이 진행되지 않았다\n",
    "\n",
    "아래의 데이터셋 준비는 shuffle이 진행되어 이전보다 조금 더 나아진 정확도를 볼 수 있다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### custom FFNN\n",
    "#### 2) 여러개의 layer가 있는 FFNN\n",
    "이제 여러개의 layer가 있는 FFNN을 고려해보자.\n",
    "$$\\begin{aligned}\n",
    "s_1 &= x \\cdot w_1 + b_1,  &(b, h_1^1)\\\\\n",
    "h_1 &= f_1(s_1), &(b, h_1^1) \\\\\n",
    "s_2 &= h_1 \\cdot w_2 + b_2,  &(b, h_2^1) \\\\\n",
    "h_2 &= f_2(s_2),  &(b, h_2^1) \\\\\n",
    "s_3 &= h_2 \\cdot w_3 + b_3,  &(b, h_3^1 = 1) \\\\\n",
    "\\hat y &= f(s_3),  &(b, 1) \\\\\n",
    "e &= \\sum_i (\\hat y - y)^2, &(1, ) \\\\\n",
    "& \\\\\n",
    "\\cfrac {\\partial e}{\\partial w_3} &= \\cfrac{\\partial e}{\\partial \\hat y} \\cfrac{\\partial \\hat y}{\\partial s_3} \\cfrac{\\partial s_3}{\\partial w_3}, &(h_2^1, h_3^1=1) \\\\\n",
    "&= h_2^T \\cdot \\left[ 2(\\hat y - y) \\times f^{'}(s_3) \\right]\\\\\n",
    "(h_2^1, h_3^1=1) &= (b, h_2^1)^T \\cdot [(b, 1) \\times (b, 1)]\\\\\n",
    "& \\\\\n",
    "\\cfrac {\\partial e}{\\partial h_2} &= \\cfrac{\\partial e}{\\partial \\hat y} \\cfrac{\\partial \\hat y}{\\partial s_3} \\cfrac{\\partial s_3}{\\partial h_2}, &(b, h_2^1) \\\\\n",
    "&= \\left[ 2(\\hat y - y) \\times f^{'}(s_3) \\right] \\cdot w_3 \\\\\n",
    "(b, h_2^1) &= [(b, 1) \\times (b, 1)] \\cdot (h_3^1=1, h_2^1)\\\\\n",
    "& \\\\\n",
    "\\cfrac {\\partial e}{\\partial w_2} &= \\cfrac {\\partial e}{\\partial h_2} \\cfrac{\\partial h_2}{\\partial s_2} \\cfrac{\\partial s_2}{\\partial w_2}, &(h_1^1, h_2^1) \\\\\n",
    "&= h_1^T \\cdot \\left[ \\cfrac {\\partial e}{\\partial h_2} \\times f_2^{'}(s_2) \\right]\\\\\n",
    "(h_1^1, h_2^1) &= (b, h_1^1)^T \\cdot [(b, h_2^1) \\times (b, h_2^1)]\\\\\n",
    "& \\\\\n",
    "\\text{for b, }& \\text{We assume } b \\cdot x_0, \\text{and always } x_0 = 1\\\\\n",
    "\\cfrac {\\partial e}{\\partial b_2} &= \\cfrac {\\partial e}{\\partial h_2} \\cfrac{\\partial h_2}{\\partial s_2} \\cfrac{\\partial s_2}{\\partial b_2}, &(h_2^1,) \\\\\n",
    "&= \\sum_b \\left[ \\cfrac {\\partial e}{\\partial h_2} \\times f_2^{'}(s_2) \\right]\\\\\n",
    "(h_2^1,) &= (b, 1)^T \\cdot [(b, h_2^1) \\times (b, h_2^1)]\n",
    "\\end{aligned}$$\n",
    "\n",
    "따라서 일반화를 하면:\n",
    "$$\\begin{aligned}\n",
    "\\cfrac {\\partial e}{\\partial w_i} &= \\cfrac {\\partial e}{\\partial h_i} \\cfrac{\\partial h_i}{\\partial s_i} \\cfrac{\\partial s_i}{\\partial w_i}, &(h_{i-1}^1, h_i^1) \\\\\n",
    "&= h_{i-1}^T \\cdot \\left[ \\cfrac {\\partial e}{\\partial h_i} \\times f_i^{'}(s_i) \\right]\\\\\n",
    "(h_{i-1}^1, h_i^1) &= (b, h_{i-1}^1)^T \\cdot [(b, h_i^1) \\times (b, h_i^1)]\\\\\n",
    "& \\\\\n",
    "\\cfrac {\\partial e}{\\partial h_i} &= \\cfrac{\\partial e}{\\partial h_{i+1}} \\cfrac{\\partial h_{i+1}}{\\partial s_{i+1}} \\cfrac{\\partial s_{i+1}}{\\partial h_i}, &(b, h_2^1) \\\\\n",
    "&= \\left[ \\cfrac{\\partial e}{\\partial h_{i+1}}  \\times f^{'}(s_3) \\right] \\cdot w_3 \\\\\n",
    "(b, h_i^1) &= [(b, h_{i+1}^1) \\times (b, h_{i+1}^1)] \\cdot (h_{i+1}^1, h_i^1)\\\\\n",
    "& \\\\\n",
    "\\cfrac {\\partial e}{\\partial b_i} &= \\cfrac {\\partial e}{\\partial h_i} \\cfrac{\\partial h_i}{\\partial s_i} \\cfrac{\\partial s_i}{\\partial b_i}, &(h_i^1, ) \\\\\n",
    "&= \\sum_b \\left[ \\cfrac {\\partial e}{\\partial h_i} \\times f_i^{'}(s_i) \\right]\\\\\n",
    "(h_i^1,) &= (b, 1)^T \\cdot [(b, h_i^1) \\times (b, h_i^1)]\n",
    "\\end{aligned}$$\n",
    "- $b_i$를 구할 때, np.sum(arr, axis=-1)을 사용하면 된다.\n",
    "\n",
    "이를 위해 propagate_forward(self, x)함수를 구해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numpy.random import shuffle, rand\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.random.rand(1024,2)\n",
    "y_train = np.array( [ 3 * x[0] - 1.2 * x[1] + .5 for x in x_train ] )\n",
    "\n",
    "x_val = np.random.rand(32,2)\n",
    "y_val = np.array( [ 3 * x[0] - 1.2 * x[1] + .5 for x in x_val ] )\n",
    "\n",
    "x_test = np.array( [ [0.2, 0.1], [0.3, 0.1], [0.4, 0.1], [0.5, 0.1] ] )\n",
    "y_test = np.array( [ 3 * x[0] - 1.2 * x[1] + .5 for x in x_test ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이를 위해 multi layer를 구현할 수 있도록 다음과 같은 절차에 따라 작성한다.\n",
    "\n",
    "1. layer를 추가할 수 있는 add 함수를 구현한다.\n",
    " - 인자로, input의 size와 activation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFNN:\n",
    "    \n",
    "    def __init__(self, lr=0.01):\n",
    "        self.lr = lr # learning rate\n",
    "        # weights list\n",
    "        self.ws = []; self.bs = []\n",
    "        self.fs = []; self.f_derivs = []\n",
    "        self.N_layers = 0\n",
    "        \n",
    "    # layer를 추가하며, weights를 초기화하기\n",
    "    def add(self, units, activation=None, activation_deriv=None, input_dim=None):\n",
    "        self.ws.append(np.random.rand(input_dim * units).reshape(input_dim, units))\n",
    "        self.bs.append(0.5)\n",
    "        self.fs.append(activation)\n",
    "        self.f_derivs.append(activation_deriv)\n",
    "        self.N_layers += 1\n",
    "        \n",
    "    # feed forwarding\n",
    "    def propagate_forward(self, x):\n",
    "        s = [x]; o = [x] # for l = 0\n",
    "        for idx in range(self.N_layers):\n",
    "            s_input = np.dot(x, self.ws[idx]) + self.bs[idx]\n",
    "            y = self.fs[idx](s_input)\n",
    "            x = y\n",
    "            o.append(y)\n",
    "            s.append(s_input)\n",
    "        return s, o\n",
    "\n",
    "    # predicting\n",
    "    def predict(self, x):\n",
    "        s, o = self.propagate_forward(x)\n",
    "        return o[-1] # output from the last layer\n",
    "\n",
    "    # train for one batch. x 자체가 batch\n",
    "    def train_on_batch(self, x_batch, y_batch, istrain=True):\n",
    "        s, o = self.propagate_forward(x_batch)\n",
    "        Y = o[-1]\n",
    "        N = x_batch.shape[0]\n",
    "        loss = np.sum((Y - y_batch))\n",
    "        if istrain:\n",
    "            qq = (2 * (Y - y_batch) * self.f_derivs[-1](s[-1]) )\n",
    "            \n",
    "            print(qq)\n",
    "#             dw3 = np.dot(o[-2], )\n",
    "#             print(dw3)    \n",
    "        return loss\n",
    "\n",
    "    # epochs에 대해 batch 별로 학습하기\n",
    "    def fit(self, x, y, batch_size, epochs, validation_data):\n",
    "        Losses = [] # validation loss after each epoch\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            Loss = 0\n",
    "            N = x.shape[0]\n",
    "            for j in range(0, N, batch_size):\n",
    "                x_batch = x[j:j+batch_size]\n",
    "                y_batch = y[j:j+batch_size]\n",
    "                n = x_batch.shape[0]\n",
    "                Loss += (self.train_on_batch(x_batch, y_batch) / n)\n",
    "        \n",
    "            print(\"Train Loss at Epoch %d is %.8f\" %(i, Loss))\n",
    "            Losses[\"train_loss\"].append(Loss)                \n",
    "\n",
    "        return Losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다양한 activation 함수 구현:\n",
    "$$\\begin{aligned}\n",
    "\\text{Linear}(x) &= x\\\\\n",
    "\\cfrac {\\partial \\text{Linear}(x)}{\\partial x} &= 1\\\\\n",
    "\\text{Sigmoid}(x) &= \\sigma (x) = \\cfrac {1}{1-\\exp(-x)}\\\\\n",
    "\\cfrac {\\partial \\text{Sigmoid}(x)}{\\partial x} &= \\sigma(x) * (1-\\sigma(x))\\\\\n",
    "\\text{tahn}(x) &= \\cfrac {\\exp(x) - \\exp(-x)}{\\exp(x) + \\exp(-x)}\\\\\n",
    "\\cfrac {\\partial \\text{tahn}(x)}{\\partial x} &= 1-\\text{tahn}^2(x)\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear(x): # linear y = x\n",
    "    return x\n",
    "\n",
    "def linear_deriv(x): # derivative of y = x\n",
    "    return 1\n",
    "\n",
    "def sigmoid(x): # sigmoid\n",
    "    return 1. / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_deriv(x): # derivative of sigmoid\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "def tanh(x): # hyperbolicv tangent\n",
    "    return np.tanh(x)\n",
    "\n",
    "def tanh_deriv(x): # derivative of hyperbolic tangent\n",
    "    return 1 - np.tanh(x) ** 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 > 3 > 3 > 1 layer를 갖는 FFNN의 인스턴스를 생성한다.\n",
    "- 출력 2개의 선형 활성함수를 갖는 첫 hidden layer 추가\n",
    "- 출력 3개의 sigmoid 활성함수를 갖는 둘째 hidden layer 추가\n",
    "- 출력 3개의 tanh 활성함수를 갖는 셋째 hidden layer 추가\n",
    "- 출력 1개의 선형 활성함수를 갖는 출력 layer 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "fnn = FFNN()\n",
    "fnn.add(2, activation=linear, activation_deriv=linear_deriv, input_dim=2)\n",
    "fnn.add(3, activation=sigmoid, activation_deriv=sigmoid_deriv, input_dim=2)\n",
    "fnn.add(3, activation=tanh, activation_deriv=tanh_deriv, input_dim=3)\n",
    "fnn.add(1, activation=linear, activation_deriv=linear_deriv, input_dim=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-48-fa27da82b84f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mLosses\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mx_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-45-8bcd41c8f335>\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, validation_data)\u001b[0m\n\u001b[0;32m     57\u001b[0m                 \u001b[0my_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m                 \u001b[0mn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx_batch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m                 \u001b[0mLoss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     60\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Train Loss at Epoch %d is %.8f\"\u001b[0m \u001b[1;33m%\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLoss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-45-8bcd41c8f335>\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x_batch, y_batch, istrain)\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[1;31m#             qq = (2 * (Y - y_batch) * self.f_derivs[-1](s[-1]) )\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m             \u001b[0mqq\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf_derivs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mqq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     44\u001b[0m \u001b[1;31m#             dw3 = np.dot(o[-2], )\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[1;31m#             print(dw3)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'int' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "Losses = fnn.fit(x_train, y_train, 20, 1000, (x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_pred\n",
      "[[0.97862965]\n",
      " [0.9794233 ]\n",
      " [0.98018522]\n",
      " [0.9809166 ]]\n",
      "y_test\n",
      "[0.98 1.28 1.58 1.88]\n"
     ]
    }
   ],
   "source": [
    "y_pred = fnn.predict(x_test)\n",
    "print('y_pred'); print(y_pred)\n",
    "print('y_test'); print(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "losses의 결과를 이전과 같이 시각화하라."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

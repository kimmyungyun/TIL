# 머신러닝

## SGD

- 임의이 T값을 기준으로 미분값을 구해서 미분값의 반대방향으로 이동하면서 미분값이 0이 되는 구간으로 간다

- 운 나쁘면 local minimum에 도착하고 운이 좋으면 global minimum에 도착한다

- ![](https://latex.codecogs.com/gif.latex?w(t&plus;1)&space;=&space;w(t)&space;-&space;{\partial\over\partial&space;w}\mathbf{Loss(w)})



## 모멘텀

- 



## Nesterov 모멘텀

- 



## RMSprop

- 학습 비율이 학습이 진행됨에 따라 자동으로 줄어드는 학습방법
- 
- 


## 머신러닝 환경설정 이유

- 2의 x승으로 미니배치 사이즈를 잡는데 그 이유는 메모리 할당이 되지 않는 구간이 생길 가능성이 높아서 메모리 낭비를 줄이기 위해
- 데이터를 셔플하는 이유는 셔플하는 것만으로도 새로운 데이터로 학습하는 것과 같은 효과가 나타나기 때문

- 학습 비율(lr)
  - SGD의 위의 수식에서 Loss 미분 부분이 너무 커져서 global minimum에 수렴하지 않는 경우를 줄이기 위해서



## 활성함수

- 시그모이드 함수
  - 양 끝점에서의 변화량이 굉장히 작기 때문에 학습이 제대로 되지 않는 경우가 있다
  - 0~1 사이의 값에서 변화량이 크기 때문에 이미지 같은 경우도 0~1사이로 변환시킨다



## 하이퍼파라미터

- 학습 비율(lr - learning rate)
- 배치사이즈
- 모멘텀 값
  - 모멘텀 방식을 사용할 때

